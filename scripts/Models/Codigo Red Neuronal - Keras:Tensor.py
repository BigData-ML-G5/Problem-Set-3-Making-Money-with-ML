# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QOmQBOpapyMEBKVJtvjQtwFQVZ3GDEHO
"""

# =======================================================
# Red Neuronal (Keras/TensorFlow) - Optimizado para Regresión
# ADAPTADO PARA GOOGLE COLAB: No requiere instalación de librerías.
# =======================================================

# ------------------------
# 0) Importar librerías
# ------------------------
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, initializers, optimizers, callbacks
import warnings
warnings.filterwarnings('ignore') # Ignorar advertencias de Keras/TensorFlow

# Fijar semillas aleatorias para garantizar que los resultados sean los mismos cada vez que se ejecute.
np.random.seed(2025)
tf.random.set_seed(2025)

# ------------------------
# 1) Cargar datos
# ------------------------
print("Cargando datos...")
# IMPORTANTE para Colab: Los archivos deben estar subidos directamente al directorio de la sesión.
# Se asume que los archivos son 'data_train_text_finished.csv' y 'data_test_text_finished.csv'
try:
    train_df = pd.read_csv(
        "train_unified_final.csv",
        na_values=["", "NA", "NaN"]
    )

    test_df = pd.read_csv(
        "test_unified_final.csv",
        na_values=["", "NA", "NaN"]
    )
except FileNotFoundError:
    print("❌ ERROR: Asegúrate de que los archivos 'data_train_text_finished.csv' y 'data_test_text_finished.csv' están cargados en el panel de archivos de Colab.")
    # Si estás ejecutando localmente, cambiar las rutas a "data/nombre_archivo.csv"
    raise

# Asegurar que la columna 'price' sea numérica y manejar valores no válidos (si los hay)
train_df['price'] = pd.to_numeric(train_df['price'], errors='coerce')

# Eliminar la columna 'property_type' si existe
if 'property_type' in train_df.columns:
    train_df = train_df.drop('property_type', axis=1)
if 'property_type' in test_df.columns:
    test_df = test_df.drop('property_type', axis=1)

# ------------------------
# 2) Preprocesamiento
# ------------------------
print("Preprocesando datos...")

# Separar variables: la meta (precio) y las características
y_train_raw = train_df['price'].values
# MEJORA 1: Transformación Logarítmica del objetivo (precio)
# Esto ayuda a la red a converger mejor cuando los valores son muy grandes.
y_train = np.log1p(y_train_raw)

train_features = train_df.drop(['price', 'property_id'], axis=1)
test_features = test_df.drop(['property_id'], axis=1)
test_property_ids = test_df['property_id'].values

# 2.1) Identificar columnas
numeric_features = train_features.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = train_features.select_dtypes(include=['object']).columns.tolist()

# 2.2) Imputación (manejo de valores faltantes)
# Impute NaNs for numeric features in training set
train_num = train_features[numeric_features].fillna(train_features[numeric_features].median())

# Prepare test_features for numeric imputation
test_num_prepared = test_features.copy()

# Add any numeric columns from train_features that are missing in test_features
for col in numeric_features:
    if col not in test_num_prepared.columns:
        test_num_prepared[col] = train_features[col].median()

# Apply imputation to the prepared test numeric features
test_num = test_num_prepared[numeric_features].fillna(train_features[numeric_features].median())

# 2.3) Codificación de variables categóricas (One-Hot Encoding)
if len(categorical_features) > 0:
    # Imputar categóricas con la moda
    train_cat = train_features[categorical_features].fillna(train_features[categorical_features].mode().iloc[0])
    test_cat = test_features[categorical_features].fillna(train_features[categorical_features].mode().iloc[0])

    # Convertir categorías a columnas binarias (dummies)
    train_cat_encoded = pd.get_dummies(train_cat, drop_first=False)
    test_cat_encoded = pd.get_dummies(test_cat, drop_first=False)

    # Alinear columnas
    missing_cols = set(train_cat_encoded.columns) - set(test_cat_encoded.columns)
    for c in missing_cols:
        test_cat_encoded[c] = 0
    test_cat_encoded = test_cat_encoded.reindex(columns=train_cat_encoded.columns, fill_value=0)

    # Combinar
    train_processed = pd.concat([train_num, train_cat_encoded], axis=1)
    test_processed = pd.concat([test_num, test_cat_encoded], axis=1)
else:
    train_processed = train_num
    test_processed = test_num

# 2.4) Estandarización / Normalización
scaler = StandardScaler()
X_train = scaler.fit_transform(train_processed)
X_test = scaler.transform(test_processed)

# Calcular la media del log-precio para inicialización de sesgo
y_mean = np.mean(y_train)

print(f"Dimensiones de X_train (Entrenamiento): {X_train.shape}")
print(f"Dimensiones de X_test (Prueba): {X_test.shape}")

# ------------------------
# 3) Construir y Compilar la Red Neuronal
# ------------------------
print("Construyendo y compilando modelo mejorado...")

keras.backend.clear_session()
n_inputs = X_train.shape[1]

# Definición del modelo Secuencial (MEJORADO)
model = models.Sequential([
    # CAPA OCULTA 1: 128 neuronas
    layers.Dense(128, activation='relu', input_shape=(n_inputs,),
                 kernel_initializer=initializers.HeNormal(seed=2025)),
    layers.Dropout(0.3),

    # CAPA OCULTA 2: 64 neuronas
    layers.Dense(64, activation='relu',
                 kernel_initializer=initializers.HeNormal(seed=2025)),
    layers.Dropout(0.2),

    # CAPA OCULTA 3: 32 neuronas
    layers.Dense(32, activation='relu',
                 kernel_initializer=initializers.HeNormal(seed=2025)),
    layers.Dropout(0.1),

    # CAPA DE SALIDA
    layers.Dense(1, activation='linear',
                 bias_initializer=initializers.Constant(value=y_mean))
])

# 4) Compilar el modelo (Integrado para asegurar ejecución)
model.compile(
    loss='mean_squared_error',
    optimizer=optimizers.Adam(learning_rate=0.001),
    metrics=['mean_absolute_error']
)

# (La compilación se ha movido a la celda anterior para asegurar que el modelo siempre esté listo)

# ------------------------
# 5) Entrenar con Callbacks
# ------------------------
print("Entrenando modelo (sobre Log-Precio)...")

early_stop = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=30,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=15,
    min_lr=1e-6,
    verbose=1
)

history = model.fit(
    X_train,
    y_train, # Entrenamos con el objetivo logarítmico
    epochs=500,
    batch_size=128,
    validation_split=0.2,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

total_epochs = len(history.history['loss'])
final_val_mae = min(history.history['val_mean_absolute_error'])
print(f"\nEntrenamiento detenido en la época: {total_epochs}")
# Nota: El MAE aquí está en escala logarítmica, por lo que será un número pequeño (ej. 0.2)
print(f"Mejor MAE de validación (Escala Logarítmica): {final_val_mae:.4f}")

# ------------------------
# 6) Generar Predicciones y Archivo de Envío
# ------------------------
print("\nGenerando predicciones...")

# 1. Predecir en escala logarítmica
pred_test_log = model.predict(X_test, verbose=0).flatten()

# 2. Invertir la transformación (exponencial) para volver a pesos reales
# np.expm1 es la inversa de np.log1p
pred_test_nn = np.expm1(pred_test_log)

# Redondear predicciones hacia el millón más cercano
pred_test_nn_round = np.floor(pred_test_nn / 1e6) * 1e6

# Crear el DataFrame de envío
submission_nn = pd.DataFrame({
    'property_id': test_property_ids,
    'price': pred_test_nn_round.astype(int)
})

# Guardar en CSV
fname_nn = "NN_keras_colab_submission.csv"
submission_nn.to_csv(fname_nn, index=False)

print(f"\n✅ Archivo de envío guardado como: {fname_nn}")
print(f"Puedes descargarlo desde el panel de archivos de Colab.")
print(f"Predicción media: ${pred_test_nn_round.mean():,.0f}")

